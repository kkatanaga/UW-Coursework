# -*- coding: utf-8 -*-
"""COMP 4730 ML Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fdgBvIPQGDMy2wTktheBZCTtIr28PLbn

# 1 - Data Preprocessing
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("valakhorasani/gym-members-exercise-dataset")

# print("Path to dataset files:", path)

"""2 - Read csv here"""

import pandas as pd

data = pd.read_csv(path + "/gym_members_exercise_tracking.csv")

# sample = data.head(10)

# print(sample)

"""3 - Convert classification features to numeric data (currently using panda dummies, worthwhile to experiment with one-hot encoding)"""

dummied_data = data

dummied_data = pd.concat([pd.get_dummies(dummied_data['Gender']), dummied_data], axis=1).drop(['Gender'], axis=1)
dummied_data = pd.concat([pd.get_dummies(dummied_data['Workout_Type']), dummied_data], axis=1).drop(['Workout_Type'], axis=1)

# print(dummied_data.info())

data_conv = dummied_data

"""Visual heatmap of feature correlations. Perhaps dropping features which have small absolute correlation with Calories_Burned will help our models. It may be good to experiment with how small a correlation must be to be considered noise."""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12,8))

sns.heatmap(data_conv.corr().sort_values(by="Calories_Burned"), annot=True, cmap="YlGnBu", fmt=".2f")

data_conv.hist(figsize=(12,12))

"""4 - Define X and Y and other variables common throughout all models"""

# from sklearn.model_selection import train_test_split,cross_val_score

# x, y = data_conv.drop(columns=['Calories_Burned']), data_conv['Calories_Burned']

# #x, y = data_conv[['Session_Duration (hours)']], data_conv['Calories_Burned']

# x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7)

"""Create 2 pipelines dedicated to
1. Linear models
2. Tree-based models
"""

# Copied from https://scikit-learn.org/1.5/auto_examples/ensemble/plot_stack_predictors.html#sphx-glr-auto-examples-ensemble-plot-stack-predictors-py
from sklearn.compose import make_column_selector, make_column_transformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline

from sklearn.model_selection import train_test_split

import numpy as np

cat_selector = make_column_selector(dtype_include=object)
num_selector = make_column_selector(dtype_include=np.number)

# cat_selector(X)
# num_selector(X)

# Create linear model preprocessor
linear_processor_cat = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
linear_processor_num = make_pipeline(
    StandardScaler(), SimpleImputer(strategy="mean", add_indicator=True)
)

linear_preprocessor = make_column_transformer(
    (linear_processor_num, num_selector), (linear_processor_cat, cat_selector)
)

# linear_preprocessor

# Create tree-based model preprocessor
tree_processor_cat = OrdinalEncoder(
    handle_unknown="use_encoded_value",
    unknown_value=-1,
    encoded_missing_value=-2,
)
tree_processor_num = SimpleImputer(strategy="mean", add_indicator=True)

tree_preprocessor = make_column_transformer(
    (tree_processor_num, num_selector), (tree_processor_cat, cat_selector)
)

# tree_preprocessor

"""## Linear Model Preprocessing"""

linear_preprocessor.set_output(transform='pandas')
# print(data.sort_values('Session_Duration (hours)'))
l_processed_data = linear_preprocessor.fit_transform(data)
# print(linear_processed_data.sort_values('pipeline__Session_Duration (hours)'))

l_processed_X = l_processed_data.loc[:, l_processed_data.columns != 'pipeline__Calories_Burned']
l_processed_y = l_processed_data['pipeline__Calories_Burned'].rename('Calories_Burned')

l_train_X, l_test_X, l_train_y, l_test_y = train_test_split(l_processed_X, l_processed_y, train_size=0.8)

# _train_join = l_train_X.join(l_train_y)
# _train_join.sort_values(univariate_X_col_name, inplace=True)
# l_train_X = _train_join.loc[:, _train_join.columns != 'Calories_Burned']
# l_train_y = _train_join.loc[:, 'Calories_Burned']

# _test_join = l_test_X.join(l_test_y)
# _test_join.sort_values(univariate_X_col_name, inplace=True)
# l_test_X = _test_join.loc[:, _test_join.columns != 'Calories_Burned']
# l_test_y = _test_join.loc[:, 'Calories_Burned']

"""## Tree-Based Model Preprocessing"""

tree_preprocessor.set_output(transform='pandas')
t_processed_data = tree_preprocessor.fit_transform(data)

t_processed_X = t_processed_data.loc[:, t_processed_data.columns != 'simpleimputer__Calories_Burned']
t_processed_y = t_processed_data['simpleimputer__Calories_Burned'].rename('Calories_Burned')

t_train_X, t_test_X, t_train_y, t_test_y = train_test_split(t_processed_X, t_processed_y, train_size=0.8)

# _train_join = t_train_X.join(t_train_y)
# _train_join.sort_values(univariate_X_cot_name, inplace=True)
# t_train_X = _train_join.loc[:, _train_join.columns != 'Calories_Burned']
# t_train_y = _train_join.loc[:, 'Calories_Burned']

# _test_join = t_test_X.join(t_test_y)
# _test_join.sort_values(univariate_X_cot_name, inplace=True)
# t_test_X = _test_join.loc[:, _test_join.columns != 'Calories_Burned']
# t_test_y = _test_join.loc[:, 'Calories_Burned']

"""## Find best feature & get feature name"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

# Univariate feature selection
#   https://scikit-learn.org/1.5/modules/feature_selection.html
#   https://stackoverflow.com/questions/39839112/the-easiest-way-for-getting-feature-names-after-running-selectkbest-in-scikit-le

def get_best_feature_name(processed_X, processed_y):
    # Create and fit feature selector
    univariate_selector = SelectKBest(f_regression, k=1)
    univariate_selector.fit(processed_X, processed_y)

    # Get column name of the best feature
    univariate_col_mask = univariate_selector.get_support()
    univariate_X_col_name = processed_X.columns[univariate_col_mask][0]
    return univariate_X_col_name

def get_positive_feature_names(processed_X, processed_y):
    # Create and fit feature selector
    multivariate_selector = SelectKBest(f_regression, k=12)  # 12 best features; feature correlation heatmap has 12 positively correlated features
    multivariate_selector.fit(processed_X, processed_y)

    # Get column name of the best feature
    multivariate_col_mask = multivariate_selector.get_support()
    multivariate_X_col_name = processed_X.columns[multivariate_col_mask][0]
    return multivariate_X_col_name

l_best_feat_name = get_best_feature_name(l_processed_X, l_processed_y)
t_best_feat_name = get_best_feature_name(t_processed_X, t_processed_y)

# l_univariate_selector = SelectKBest(f_classif, k=1)
# l_univariate_selector.fit(l_processed_X, l_processed_y)
# t_univariate_selector = SelectKBest(f_classif, k=1)
# t_univariate_selector.fit(t_processed_X, t_processed_y)

# univariate_cols_idxs = l_univariate_selector.get_support(indices=True)
# univariate_X = l_processed_X.iloc[:,univariate_cols_idxs] #['pipeline__Session_Duration (hours)'].rename('Session_Duration (hours)')
# l_univariate_col_mask = l_univariate_selector.get_support()
# l_univariate_X_col_name = l_processed_X.columns[l_univariate_col_mask][0]
# t_univariate_col_mask = t_univariate_selector.get_support()
# t_univariate_X_col_name = t_processed_X.columns[t_univariate_col_mask][0]

"""# Training"""

from sklearn.model_selection import GridSearchCV, KFold
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor

from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import PredictionErrorDisplay

from time import process_time
import tracemalloc

CV_COUNT = 5

"""## Linear Models

Linear Models used:


1.   Linear Regression
2.   SVR

### Hyperparameter Tuning (find best hyperparameters exhaustively)

Tuning with All Features
"""

def l_tune_exhaustively_and_show(model, param_grid, model_name):
    return tune_exhaustively_and_show(model, param_grid, model_name, l_train_X, l_train_y, l_test_X, l_test_y)

def t_tune_exhaustively_and_show(model, param_grid, model_name):
    return tune_exhaustively_and_show(model, param_grid, model_name, t_train_X, t_train_y, t_test_X, t_test_y)

def tune_exhaustively_and_show(model, param_grid, model_name, train_X, train_y, test_X, test_y):
    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=CV_COUNT, scoring='r2').fit(train_X, train_y)
    best = grid.best_estimator_

    print(f'---{model_name} Results---')
    print('Best hyperparameters using grid search are ', grid.best_params_)
    print('Best model found: ', best)
    print('Best score: ', best.score(test_X, test_y))

"""Tuning with Single Input Feature"""

def l_univariate_tune_exhaustively_and_show(model, param_grid, model_name):
    return tune_exhaustively_and_show(model, param_grid, model_name, l_train_X.loc[:, [l_best_feat_name]], l_train_y, l_test_X.loc[:, [l_best_feat_name]], l_test_y)

def t_univariate_tune_exhaustively_and_show(model, param_grid, model_name):
    return tune_exhaustively_and_show(model, param_grid, model_name, t_train_X.loc[:, [t_best_feat_name]], t_train_y, t_test_X.loc[:, [t_best_feat_name]], t_test_y)

"""Linear Models"""

# Skip to the next code block if best hyperparameters are already found

# lin_parameters = {'fit_intercept': [True, False], 'positive': [False, True]}
# svr_parameters = {'C': [0.5, 1.0, 5.0, 10.0, 25.0, 50.0, 75.0, 100.0], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], 'degree': [2, 3, 4], 'gamma': ['scale', 'auto']}

# lin_tuned = l_tune_exhaustively_and_show(LinearRegression(), lin_parameters, 'Linear Regression')
# svr_tuned = l_tune_exhaustively_and_show(SVR(), svr_parameters, 'SVR')

# lin_uni_tuned = l_univariate_tune_exhaustively_and_show(LinearRegression(), lin_parameters, 'Linear Regression')
# svr_uni_tuned = l_univariate_tune_exhaustively_and_show(SVR(), svr_parameters, 'SVR')

"""Tree-based Models"""

dtr_parameters = {'random_state' : [42], 'criterion' : ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'], 'min_samples_split': [2, 3, 5, 10, 20, 30, 40, 50], 'min_samples_leaf': [1, 2, 3, 5, 10]}
rfr_parameters = {'random_state' : [42], 'criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'], 'min_samples_split': [2, 3, 5, 10, 20, 30, 40, 50], 'min_samples_leaf': [1, 2, 3, 5, 10]}
gbr_parameters = {'random_state' : [42], 'loss' : ['squared_error', 'absolute_error', 'huber', 'quantile'], 'learning_rate': [0.01, 0.05, 0.1, 0.2], 'subsample':[0.6, 0.85, 1.0], 'min_samples_split': [2, 3, 5, 10, 20, 30, 40, 50], 'min_samples_leaf': [1, 2, 3, 5, 10]}
# # , 'n_estimators': [100, 150, 200, 1000]

# dtr_tuned = t_tune_exhaustively_and_show(DecisionTreeRegressor(), dtr_parameters, 'Decision Tree Regressor')
# rfr_tuned = t_tune_exhaustively_and_show(RandomForestRegressor(), rfr_parameters, 'Random Forest Regressor')
gbr_tuned = t_tune_exhaustively_and_show(GradientBoostingRegressor(), gbr_parameters, 'Gradient Boosting Regressor')

dtr_uni_tuned = t_univariate_tune_exhaustively_and_show(DecisionTreeRegressor(), dtr_parameters, 'Decision Tree Regressor')
rfr_uni_tuned = t_univariate_tune_exhaustively_and_show(RandomForestRegressor(), rfr_parameters, 'Random Forest Regressor')
gbr_uni_tuned = t_univariate_tune_exhaustively_and_show(GradientBoostingRegressor(), gbr_parameters, 'Gradient Boosting Regressor')

# """### Best Hyperparameters Found

# Linear Models
# """

# lin_default     = LinearRegression()
# lin_tuned       = LinearRegression()
# lin_tuned_uni   = LinearRegression(fit_intercept=False)

# svr_default     = SVR()
# svr_tuned       = SVR(C=10.0, degree=2, gamma='auto', kernel='poly')
# # SVR(C=25.0, degree=2, gamma='auto', kernel='poly')
# svr_tuned_uni   = SVR(C=5.0, degree=2, kernel='linear')

# """Tree-based Models"""

# dtr_default     = DecisionTreeRegressor(random_state=42)
# dtr_tuned       = DecisionTreeRegressor(criterion='poisson', min_samples_leaf=3, random_state=42)
# dtr_tuned_uni   = DecisionTreeRegressor(min_samples_leaf=10, min_samples_split=50, random_state=42)

# rfr_default     = RandomForestRegressor(random_state=42)
# rfr_tuned       = RandomForestRegressor(criterion='poisson', random_state=42)  # n_estimators=200 and 1000 obviously increases the prediction score. Maybe note down by how much?
# rfr_tuned_uni   = RandomForestRegressor(criterion='poisson', min_samples_leaf=10, min_samples_split=50, random_state=42)

# gbr_default     = GradientBoostingRegressor(random_state=42)
# # gbr_tuned       = GradientBoostingRegressor(loss='huber', min_samples_leaf=3, random_state=42)      # n_estimators=200 and 1000 obviously increases the prediction score. Maybe note down by how much?
# gbr_tuned       = GradientBoostingRegressor(learning_rate=0.2, min_samples_leaf=2, min_samples_split=50, random_state=42, subsample=0.85)  #, n_estimators=1000
# gbr_tuned_uni   = GradientBoostingRegressor(learning_rate=0.05, loss='absolute_error', min_samples_leaf=10, min_samples_split=40, random_state=42, subsample=0.6)


# # rfr_tuned = RandomForestRegressor(criterion='poisson', min_samples_split=3, n_estimators=200, random_state=42)
# # rfr_tuned = RandomForestRegressor(criterion='poisson', min_samples_split=3, n_estimators=1000, random_state=42)

# # gbr_tuned = GradientBoostingRegressor(min_samples_leaf=3, n_estimators=200, random_state=42)

# """### Print Results

# Functions
# """

# def fit_score_model(model, train_X, train_y, test_X, test_y):
#     tracemalloc.start()

#     t_start = process_time()
#     model.fit(train_X, train_y)
#     t_stop = process_time()

#     fit_time = t_stop - t_start

#     t_start = process_time()
#     r2_score = model.score(test_X, test_y)
#     t_stop = process_time()

#     score_time = t_stop - t_start

#     memory_use = tracemalloc.get_traced_memory()

#     tracemalloc.stop()

#     return (r2_score, fit_time, score_time, memory_use[1] / 1000)

# def print_results(model, results_tuple, show_hyperparameters=False):
#     print(f'r2 score: {results_tuple[0]}')
#     print(f'Training time: {results_tuple[1]}s')
#     print(f'Score time: {results_tuple[2]}s')
#     print(f'Peak memory use: {results_tuple[3]}kb')
#     print(f'Model initialization: {model}')
#     if show_hyperparameters:
#         print(f'Best hyperparameters using grid search are {model.get_params()}')
#     print()



# def l_print_default_results(model, model_name):
#     results_tuple = fit_score_model(model, l_train_X, l_train_y, l_test_X, l_test_y)
#     print_default_results(model, model_name, results_tuple)
#     return results_tuple

# def l_print_tuned_results(model, model_name):
#     results_tuple = fit_score_model(model, l_train_X, l_train_y, l_test_X, l_test_y)
#     print_tuned_results(model, model_name, results_tuple)
#     return results_tuple

# def l_print_tuned_uni_results(model, model_name):
#     results_tuple = fit_score_model(model, l_train_X.loc[:, [l_best_feat_name]], l_train_y, l_test_X.loc[:, [l_best_feat_name]], l_test_y)
#     print_tuned_uni_results(model, model_name, results_tuple)
#     return results_tuple



# def t_print_default_results(model, model_name):
#     results_tuple = fit_score_model(model, t_train_X, t_train_y, t_test_X, t_test_y)
#     print_default_results(model, model_name, results_tuple)
#     return results_tuple

# def t_print_tuned_results(model, model_name):
#     results_tuple = fit_score_model(model, t_train_X, t_train_y, t_test_X, t_test_y)
#     print_tuned_results(model, model_name, results_tuple)
#     return results_tuple

# def t_print_tuned_uni_results(model, model_name):
#     results_tuple = fit_score_model(model, t_train_X.loc[:, [t_best_feat_name]], t_train_y, t_test_X.loc[:, [t_best_feat_name]], t_test_y)
#     print_tuned_uni_results(model, model_name, results_tuple)
#     return results_tuple



# def print_default_results(model, model_name, results_tuple):
#     print(f'---Default {model_name} Results---')
#     print_results(model, results_tuple)

# def print_tuned_results(model, model_name, results_tuple):
#     print(f'---Tuned {model_name} Results---')
#     print_results(model, results_tuple, True)

# def print_tuned_uni_results(model, model_name, results_tuple):
#     print(f'---Tuned Univariate {model_name} Results---')
#     print_results(model, results_tuple, True)

# res_r2_scores = {'Default' : [], 'Tuned' : [], 'Tuned Univariate' : []}
# res_train_times = {'Default' : [], 'Tuned' : [], 'Tuned Univariate' : []}
# res_test_times = {'Default' : [], 'Tuned' : [], 'Tuned Univariate' : []}
# res_mem_uses = {'Default' : [], 'Tuned' : [], 'Tuned Univariate' : []}

# RES_MATRIX = (res_r2_scores, res_train_times, res_test_times, res_mem_uses)

# def append_result(model_results, key):
#     for res_dict, res in zip(RES_MATRIX, model_results):
#         res_dict[key].append(res)

# """Linear Models"""

# append_result(l_print_default_results(lin_default, 'Linear Regression'), 'Default')
# append_result(l_print_tuned_results(lin_tuned, 'Linear Regression'), 'Tuned')
# append_result(l_print_tuned_uni_results(lin_tuned_uni, 'Linear Regression'), 'Tuned Univariate')

# append_result(l_print_default_results(svr_default, 'SVR'), 'Default')
# append_result(l_print_tuned_results(svr_tuned, 'SVR'), 'Tuned')
# append_result(l_print_tuned_uni_results(svr_tuned_uni, 'SVR'), 'Tuned Univariate')

# """Tree-based Models"""

# append_result(t_print_default_results(dtr_default, 'Decision Trees'), 'Default')
# append_result(t_print_tuned_results(dtr_tuned, 'Decision Trees'), 'Tuned')
# append_result(t_print_tuned_uni_results(dtr_tuned_uni, 'Decision Trees'), 'Tuned Univariate')

# append_result(t_print_default_results(rfr_default, 'Random Forest'), 'Default')
# append_result(t_print_tuned_results(rfr_tuned, 'Random Forest'), 'Tuned')
# append_result(t_print_tuned_uni_results(rfr_tuned_uni, 'Random Forest'), 'Tuned Univariate')

# append_result(t_print_default_results(gbr_default, 'Gradient Boosting'), 'Default')
# append_result(t_print_tuned_results(gbr_tuned, 'Gradient Boosting'), 'Tuned')
# append_result(t_print_tuned_uni_results(gbr_tuned_uni, 'Gradient Boosting'), 'Tuned Univariate')

# MODEL_NAMES = ('Linear Regression', 'Support Vector Regression', 'Decision Trees', 'Random Forest', 'Gradient Boosting')

# # Same size as RES_MATRIX
# plot_titles = ['R2 Scores', 'Training Times',   'Testing Times',    'Memory Usage'          ]
# ylabels     = ['R2 score',  'Time elapsed (s)', 'Time elapsed (s)', 'Peak memory usage (kb)']
# neg_offsets = [0.02,        1,                  1,                  0                       ]
# pos_offsets = [0.045,       0.25,               0.005,              60                      ]

# def plot_res_bar():
#     x_axis = np.arange(len(MODEL_NAMES))  # the label locations
#     width = 0.25  # the width of the bars

#     for res_type, plot_title, ylabel, neg_offset, pos_offset in zip(RES_MATRIX, plot_titles, ylabels, neg_offsets, pos_offsets):
#         multiplier = 0
#         fig, ax = plt.subplots(layout='constrained')

#         for model_variant, model_type_result in res_type.items():
#             offset = width * multiplier
#             rects = ax.bar(x_axis + offset, model_type_result, width, label=model_variant)
#             ax.bar_label(rects, padding=3, rotation=45)
#             multiplier += 1

#         res_type_vector = tuple(_b for _a in res_type.values() for _b in _a)

#         # Add some text for labels, title and custom x-axis tick labels, etc.
#         ax.set_title(f'{plot_title} by Tuning Methods and Regression Models')
#         ax.set_xticks(x_axis + width, MODEL_NAMES, rotation=45)
#         ax.set_ylabel(ylabel)
#         ax.legend(loc='upper left', ncols=3)
#         ax.set_ylim(max(0, min(res_type_vector) - neg_offset), max(res_type_vector) + pos_offset)

#         plt.show()

# plot_res_bar()

# print(t_print_tuned_results(GradientBoostingRegressor(learning_rate=0.2, min_samples_split=60, n_estimators=1000, random_state=42, subsample=0.85), 'Gradient Boosting'))
# print(t_print_tuned_results(GradientBoostingRegressor(learning_rate=0.2, min_samples_split=60, random_state=42, subsample=0.85), 'Gradient Boosting'))

# """### Visualize Best Results of Each Model Type

# Prediction Error Display
# """

# # https://scikit-learn.org/stable/visualizations.html
# # svr_pred_y = svr_best.predict(l_test_X)
# # svr_error_disp = PredictionErrorDisplay(y_true=l_test_y, y_pred=svr_pred_y)
# # svr_error_disp.plot(kind='actual_vs_predicted')
# # svr_error_disp.plot()

# gbr_pred_y = gbr_best.predict(t_test_X)
# gbr_error_disp = PredictionErrorDisplay(y_true=t_test_y, y_pred=gbr_pred_y)
# gbr_error_disp.plot(kind='actual_vs_predicted')
# gbr_error_disp.plot()

# # How to read plots: https://scikit-learn.org/stable/modules/model_evaluation.html#visualization-regression-evaluation

# """Old plot code"""

# # univariate_X = l_test_X[l_best_feat_name]
# # predict_y = svr_best.predict(l_test_X)
# # # predict_y = SVR(kernel='linear').fit(train_X, train_y).predict(test_X)

# # univar_df = univariate_X.to_frame().reset_index(drop=True)
# # predict_y_df = pd.DataFrame(predict_y, columns=['Calories_Burned_P']).reset_index(drop=True)
# # test_y_df = l_test_y.to_frame().reset_index(drop=True)

# # joined_df = univar_df.join(predict_y_df).join(test_y_df).sort_values(l_best_feat_name)

# # lw = 2
# # plt.plot(
# #         joined_df[l_best_feat_name],
# #         joined_df['Calories_Burned_P'],
# #         color='c',
# #         lw=lw,
# #         label="Model Vector"
# #     )
# # plt.scatter(
# #         joined_df[l_best_feat_name],
# #         joined_df['Calories_Burned'],
# #         facecolor='m',
# #         # edgecolor='r',
# #         s=25,
# #         label="Model Prediction",
# #     )
# # plt.show()

# """# Etc

# ## Random Forest regressor
# """

# # Import necessary libraries
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.metrics import mean_squared_error, r2_score

# # Initialize models
# random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)


# # Train Random Forest Regressor
# random_forest_model.fit(x_train, y_train)
# rf_predictions = random_forest_model.predict(x_test)

# # Evaluate Random Forest Regressor
# rf_mse = mean_squared_error(y_test, rf_predictions)
# rf_r2 = r2_score(y_test, rf_predictions)
# print("Random Forest Regressor Performance:")
# print("Mean Squared Error:", rf_mse)
# print("R^2 Score:", rf_r2)

# """## Gradient Boosting Regressor"""

# # Import necessary libraries
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.metrics import mean_squared_error, r2_score

# # Initialize models
# gradient_boosting_model = GradientBoostingRegressor(n_estimators=1000, min_samples_split=50, min_samples_leaf=10, subsample=0.85, learning_rate=0.1, random_state=42)
# #n_estimators larger than 100 decreases mean squared error significantly, anything larger than 1000 is negligible.

# # Train Gradient Boosting Regressor
# gradient_boosting_model.fit(x_train, y_train)
# gb_predictions = gradient_boosting_model.predict(x_test)

# # Evaluate Gradient Boosting Regressor
# gb_mse = mean_squared_error(y_test, gb_predictions)
# gb_r2 = r2_score(y_test, gb_predictions)
# print("\nGradient Boosting Regressor Performance:")
# print("Mean Squared Error:", gb_mse)
# print("R^2 Score:", gb_r2)

# param_grid = [
#   {'n_estimators':[1000], 'subsample':[0.85, 1.0], 'random_state':[42], 'min_samples_leaf':[10], 'min_samples_split':[40, 50, 60, 65]},
#  ]

# gb_grid = GridSearchCV(estimator=GradientBoostingRegressor(), param_grid=param_grid, cv=5, scoring='r2')

# gb_grid.fit(x_train, y_train)

# gb_predictions = gb_grid.predict(x_test)

# gb_mse = mean_squared_error(y_test, gb_predictions)
# gb_r2 = r2_score(y_test, gb_predictions)
# print("\nGradient Boosting Regressor Performance:")
# print("Mean Squared Error:", gb_mse)
# print("R^2 Score:", gb_r2)

# print(gb_grid.best_params_)

# #{'min_samples_leaf': 10, 'min_samples_split': 50, 'n_estimators': 1000, 'subsample': 0.85}
# #MSE = 300.96199965106655

# #MSE=341

# """## Gaussian Naive Bayes"""

# from sklearn.naive_bayes import GaussianNB
# import numpy as np

# gnb = GaussianNB()
# y_pred = gnb.fit(x_train, y_train).predict(x_test)
# print("Average Percent Error: ", np.average(np.abs(y_pred - y_test)/y_test * 100))